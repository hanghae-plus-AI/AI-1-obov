{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /Users/obov/Downloads/aihub_sample/신규_한영_예술체육학_sample.xlsx\n",
      "Processing file: /Users/obov/Downloads/aihub_sample/신규_한영_의약학_sample.xlsx\n",
      "Processing file: /Users/obov/Downloads/aihub_sample/신규_한영_자연과학_sample.xlsx\n",
      "Processing file: /Users/obov/Downloads/aihub_sample/신규_한영_복합학_sample.xlsx\n",
      "Processing file: /Users/obov/Downloads/aihub_sample/신규_한영_사회과학_sample.xlsx\n",
      "Processing file: /Users/obov/Downloads/aihub_sample/신규_한영_공학_sample.xlsx\n",
      "Processing file: /Users/obov/Downloads/aihub_sample/신규_한영_인문학_sample.xlsx\n",
      "Processing file: /Users/obov/Downloads/aihub_sample/신규_한영_농수해양학_sample.xlsx\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 19098\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 1009\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 1062\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
    "\n",
    "\n",
    "def convert_excel_to_dataset(file_path, val_size=0.05, test_size=0.05):\n",
    "    \"\"\"\n",
    "    엑셀 파일을 pandas로 읽어 train, validation, test Dataset으로 나누는 함수\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(file_path, engine=\"openpyxl\")\n",
    "\n",
    "    # 엑셀 데이터를 translation 구조로 변환\n",
    "    translations = []\n",
    "    for index, row in df.iterrows():\n",
    "        translations.append(\n",
    "            {\n",
    "                \"translation\": {\n",
    "                    \"ko\": row[\n",
    "                        \"SOURCE_SENTENCE\"\n",
    "                    ],  # 'SOURCE_SENTENCE' 컬럼이 한국어 문장\n",
    "                    \"en\": row[\"MT_SENTENCE\"],  # 'MT_SENTENCE' 컬럼이 영어 문장\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # 전체 데이터를 train, validation, test로 분리\n",
    "    full_dataset = Dataset.from_list(translations)\n",
    "\n",
    "    # 5%는 test로 나누기\n",
    "    train_valid_test_split = full_dataset.train_test_split(test_size=test_size)\n",
    "    train_valid_dataset = train_valid_test_split[\"train\"]\n",
    "    test_dataset = train_valid_test_split[\"test\"]\n",
    "\n",
    "    # 남은 train_valid에서 5%를 validation으로 나누기\n",
    "    train_valid_split = train_valid_dataset.train_test_split(test_size=val_size)\n",
    "    train_dataset = train_valid_split[\"train\"]\n",
    "    valid_dataset = train_valid_split[\"test\"]\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "\n",
    "def process_folder_with_file_splits(folder_path, val_size=0.05, test_size=0.05):\n",
    "    \"\"\"\n",
    "    폴더를 재귀적으로 탐색하면서 파일별로 train, validation, test로 나누는 함수\n",
    "    \"\"\"\n",
    "    train_datasets = []\n",
    "    valid_datasets = []\n",
    "    test_datasets = []\n",
    "\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".xlsx\") or file.endswith(\".xls\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                print(f\"Processing file: {file_path}\")\n",
    "\n",
    "                # 파일에서 바로 train, validation, test로 나누기\n",
    "                train_dataset, valid_dataset, test_dataset = convert_excel_to_dataset(\n",
    "                    file_path, val_size=val_size, test_size=test_size\n",
    "                )\n",
    "\n",
    "                # 각 세트별로 리스트에 추가\n",
    "                train_datasets.append(train_dataset)\n",
    "                valid_datasets.append(valid_dataset)\n",
    "                test_datasets.append(test_dataset)\n",
    "\n",
    "    # 각 세트들을 하나의 Dataset으로 결합\n",
    "    if train_datasets and valid_datasets and test_datasets:\n",
    "        concatenated_train_dataset = concatenate_datasets(train_datasets)\n",
    "        concatenated_valid_dataset = concatenate_datasets(valid_datasets)\n",
    "        concatenated_test_dataset = concatenate_datasets(test_datasets)\n",
    "\n",
    "        # DatasetDict로 반환\n",
    "        return DatasetDict(\n",
    "            {\n",
    "                \"train\": concatenated_train_dataset,\n",
    "                \"validation\": concatenated_valid_dataset,\n",
    "                \"test\": concatenated_test_dataset,\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"No Excel files found in the folder.\")\n",
    "\n",
    "\n",
    "# 실행 예시 (폴더 경로를 입력하여 실행)\n",
    "folder_path = \"/Users/obov/Downloads/aihub_sample\"  # 실제 폴더 경로로 변경\n",
    "dataset_dict = process_folder_with_file_splits(folder_path)\n",
    "\n",
    "# 변환된 데이터셋 정보 출력\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a20c7bb393f4b508e0b4d295ca0fa15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646673de2f1146c0a94d91b11ffc1a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9eb40a8ffd14f8c8f04921e85650b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14a33dbdf034d6f82b30205e1c57b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c3b3db34c24fbabfb7013b66f33807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be78c196b656458681570d34240c581c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa25f104dc4b4693817838dee6e033cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset uploaded to https://huggingface.co/datasets/obov/hf-aihub-sample-translate-en-ko\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, load_dataset, Dataset\n",
    "from huggingface_hub import HfApi, HfFolder, Repository, notebook_login\n",
    "\n",
    "# Hugging Face 로그인 (토큰을 설정하거나 자동으로 로그인)\n",
    "notebook_login()\n",
    "\n",
    "# 데이터셋 업로드할 준비된 데이터셋 예시\n",
    "# dataset_dict = DatasetDict({...}) <- 이전 단계에서 만든 데이터셋을 사용\n",
    "dataset_dict = dataset_dict  # 위에서 생성한 DatasetDict\n",
    "\n",
    "# 데이터셋을 업로드할 Hugging Face 데이터셋 리포지토리 이름을 설정\n",
    "dataset_repo_id = \"obov/hf-aihub-sample-translate-en-ko\"\n",
    "\n",
    "# 데이터셋 저장소를 Hugging Face에 업로드하기\n",
    "dataset_dict.push_to_hub(dataset_repo_id)\n",
    "\n",
    "print(f\"Dataset uploaded to https://huggingface.co/datasets/{dataset_repo_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week4-K7SCKvIm-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
